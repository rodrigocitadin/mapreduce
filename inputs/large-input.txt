mapreduce is a programming model for processing large data sets
mapreduce is inspired by the map and reduce functions common in functional programming
the mapreduce model consists of a map procedure which performs filtering and sorting
and a reduce method which performs a summary operation
the mapreduce system orchestrates the processing by marshalling the distributed servers
running the various tasks in parallel managing all communications and data transfers
between the various parts of the system and providing for redundancy and fault tolerance
mapreduce is a framework for processing parallelizable problems across large datasets
using a large number of computers collectively referred to as a cluster
mapreduce can take terabytes of data as input and produce petabytes of data as output
a mapreduce job usually splits the input data set into independent chunks
which are processed by the map tasks in a completely parallel manner
the framework sorts the outputs of the maps which are then input to the reduce tasks
typically both the input and the output of the job are stored in a file system
the framework takes care of scheduling tasks monitoring them and re-executes the failed tasks
