mapreduce can take terabytes of data as input and produce petabytes of data as output
a mapreduce job usually splits the input data set into independent chunks
which are processed by the map tasks in a completely parallel manner
the framework sorts the outputs of the maps which are then input to the reduce tasks
typically both the input and the output of the job are stored in a file system
the framework takes care of scheduling tasks monitoring them and re-executes the failed tasks
